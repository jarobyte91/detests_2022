{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89e4302f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d669b449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import torch.nn as nn\n",
    "class BetoBasic(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer, \n",
    "        model,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert = model\n",
    "        \n",
    "    def tokenize_data(self, example):\n",
    "        return self.tokenizer(example['features'], padding = 'max_length')\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        hidden = self.bert(\n",
    "            input_ids = input_ids, \n",
    "            token_type_ids = token_type_ids,\n",
    "            attention_mask = attention_mask\n",
    "        ).last_hidden_state[:, 0, :]\n",
    "        return self.output_layer(hidden)\n",
    "    \n",
    "    def predecir_base(self, X, batch_size = 2, progress_bar = False):\n",
    "        with torch.no_grad():\n",
    "            self.eval()\n",
    "#             print(\"    Generating predictions...\")\n",
    "            tokens = self.tokenizer(\n",
    "                X.tolist(), \n",
    "                padding = \"longest\", \n",
    "                truncation = True\n",
    "            )\n",
    "            p = next(self.parameters())\n",
    "            input_ids = torch.tensor(tokens[\"input_ids\"]).long()\n",
    "            token_type_ids = torch.tensor(tokens[\"token_type_ids\"]).long()\n",
    "            attention_mask = torch.tensor(tokens[\"attention_mask\"]).long()\n",
    "            dataset = tud.TensorDataset(\n",
    "                input_ids, \n",
    "                token_type_ids,\n",
    "                attention_mask,\n",
    "            )\n",
    "            loader = tud.DataLoader(dataset, batch_size = batch_size)\n",
    "            output = []\n",
    "            iterator = iter(loader)\n",
    "            if progress_bar:\n",
    "                iterator = tqdm(iterator)\n",
    "            for batch in iterator:\n",
    "                i, t, a = batch\n",
    "                predictions = self.forward(\n",
    "                    input_ids = i.to(p.device),\n",
    "                    token_type_ids = t.to(p.device),\n",
    "                    attention_mask = a.to(p.device)\n",
    "                )\n",
    "                output.append(predictions)\n",
    "            return torch.cat(output, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bebd40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetoMTL(BetoBasic):\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer, \n",
    "        model,\n",
    "    ):\n",
    "        super().__init__(tokenizer, model)\n",
    "        self.output_layer = nn.Linear(768, 10)\n",
    "        torch.save(self.state_dict(), \"clf.pt\")      \n",
    "        \n",
    "    def predecir_proba(self, X, **kwargs):\n",
    "        return self.predecir_base(X, **kwargs).sigmoid().cpu().numpy()\n",
    "    \n",
    "    def predecir(self, X, **kwargs):\n",
    "        return self.predecir_proba(X, **kwargs) > 0.5\n",
    "\n",
    "    def entrenar(\n",
    "        self, \n",
    "        X_train, \n",
    "        Y_train,\n",
    "        X_test,\n",
    "        Y_test,\n",
    "        epochs = 2, \n",
    "        batch_size = 2,\n",
    "        learning_rate = 10**-5,\n",
    "        progress_bar = True,\n",
    "        refresh = True, \n",
    "        weight_decay = 0, \n",
    "        freeze_encoder = False, \n",
    "    ):\n",
    "        assert isinstance(X_train, np.ndarray)\n",
    "        assert isinstance(Y_train, np.ndarray)\n",
    "        assert isinstance(X_test, np.ndarray)\n",
    "        assert isinstance(Y_test, np.ndarray)\n",
    "        print(\"Training model...\")\n",
    "        if refresh:\n",
    "            self.load_state_dict(torch.load(\"clf.pt\"))\n",
    "        if freeze_encoder:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        tokens = self.tokenizer(\n",
    "            X_train.tolist(), \n",
    "            padding = \"longest\", \n",
    "            truncation = True\n",
    "        )\n",
    "        p = next(self.parameters())\n",
    "        input_ids = torch.tensor(tokens[\"input_ids\"]).long()\n",
    "        token_type_ids = torch.tensor(tokens[\"token_type_ids\"]).long()\n",
    "        attention_mask = torch.tensor(tokens[\"attention_mask\"]).long()\n",
    "        label = torch.tensor(Y_train).float()\n",
    "        dataset = tud.TensorDataset(\n",
    "            input_ids, \n",
    "            token_type_ids,\n",
    "            attention_mask,\n",
    "            label\n",
    "        )\n",
    "        loader = tud.DataLoader(\n",
    "            dataset, \n",
    "            shuffle = True, \n",
    "            batch_size = batch_size\n",
    "        )\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "        training_steps = epochs * len(loader)\n",
    "        if progress_bar:\n",
    "            bar = tqdm(range(training_steps))\n",
    "        self.train()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            for j, batch in enumerate(loader):\n",
    "                i, t, a, l = batch\n",
    "                predictions = self.forward(\n",
    "                    input_ids = i.to(p.device),\n",
    "                    token_type_ids = t.to(p.device),\n",
    "                    attention_mask = a.to(p.device)\n",
    "                )\n",
    "                l = l.to(p.device)\n",
    "                loss = criterion(predictions, l)\n",
    "                accuracy = ((predictions > 0) == l).sum()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if progress_bar:\n",
    "                    bar.update(1)\n",
    "                losses.append(loss.item())\n",
    "                accuracies.append(accuracy.item())\n",
    "                if epoch == 1 and j == 0:\n",
    "                    print(f\"    first step, train loss:{loss.item():.4f}\")\n",
    "            total_loss = sum(losses) / len(losses)\n",
    "            total_accuracy = sum(accuracies) / len(X_train) / 10\n",
    "            print(f\"    epoch: {epoch}, train loss:{total_loss:.4f}, train accuracy: {total_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad37e5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xenophobia</th>\n",
       "      <th>suffering</th>\n",
       "      <th>economic</th>\n",
       "      <th>migration</th>\n",
       "      <th>culture</th>\n",
       "      <th>benefits</th>\n",
       "      <th>health</th>\n",
       "      <th>security</th>\n",
       "      <th>dehumanisation</th>\n",
       "      <th>others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3812</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3813</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3814</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3815</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3816</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3817 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      xenophobia  suffering  economic  migration  culture  benefits  health  \\\n",
       "0              0          0         0          0        0         0       0   \n",
       "1              0          0         0          0        0         0       0   \n",
       "2              0          0         0          0        0         0       0   \n",
       "3              0          0         0          0        0         0       0   \n",
       "4              0          0         0          0        0         0       0   \n",
       "...          ...        ...       ...        ...      ...       ...     ...   \n",
       "3812           0          0         0          0        0         0       0   \n",
       "3813           0          0         0          0        0         0       0   \n",
       "3814           0          0         0          0        0         0       0   \n",
       "3815           0          0         0          0        0         0       0   \n",
       "3816           0          0         0          0        0         0       0   \n",
       "\n",
       "      security  dehumanisation  others  \n",
       "0            0               0       0  \n",
       "1            0               0       0  \n",
       "2            0               0       0  \n",
       "3            0               0       0  \n",
       "4            0               0       0  \n",
       "...        ...             ...     ...  \n",
       "3812         0               0       0  \n",
       "3813         0               0       0  \n",
       "3814         0               0       0  \n",
       "3815         0               0       0  \n",
       "3816         0               0       0  \n",
       "\n",
       "[3817 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"../../data/task_2.csv\")\n",
    "\n",
    "\n",
    "\n",
    "labels = data.iloc[:, 1:]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "678f1b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"../../assets/beto/tokenizer\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0c7dd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../assets/beto/model were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModel.from_pretrained(\"../../assets/beto/model\")\n",
    "model.to(device)\n",
    "\n",
    "X = data.sentence.sample(100)\n",
    "Y = labels.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69580287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (100,)\n",
      "Y (100, 10)\n",
      "Cross-validation process started...\n",
      "*** fold 1 / 5\n",
      "    training model...\n",
      "Training model...\n",
      "    first step, train loss:0.7661\n",
      "    epoch: 1, train loss:0.3487, train accuracy: 0.9025\n",
      "    epoch: 2, train loss:0.1730, train accuracy: 0.9587\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "    Total runtime: 0.06 minutes\n",
      "*** fold 2 / 5\n",
      "    training model...\n",
      "Training model...\n",
      "    first step, train loss:0.7109\n",
      "    epoch: 1, train loss:0.3432, train accuracy: 0.9075\n",
      "    epoch: 2, train loss:0.1719, train accuracy: 0.9587\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "    Total runtime: 0.10 minutes\n",
      "*** fold 3 / 5\n",
      "    training model...\n",
      "Training model...\n",
      "    first step, train loss:0.7243\n",
      "    epoch: 1, train loss:0.3584, train accuracy: 0.8988\n",
      "    epoch: 2, train loss:0.1807, train accuracy: 0.9575\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "    Total runtime: 0.15 minutes\n",
      "*** fold 4 / 5\n",
      "    training model...\n",
      "Training model...\n",
      "    first step, train loss:0.6915\n",
      "    epoch: 1, train loss:0.3444, train accuracy: 0.9163\n",
      "    epoch: 2, train loss:0.1614, train accuracy: 0.9637\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "    Total runtime: 0.20 minutes\n",
      "*** fold 5 / 5\n",
      "    training model...\n",
      "Training model...\n",
      "    first step, train loss:0.6934\n",
      "    epoch: 1, train loss:0.3379, train accuracy: 0.9100\n",
      "    epoch: 2, train loss:0.1583, train accuracy: 0.9662\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "    Total runtime: 0.24 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>benefits</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>culture</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dehumanisation</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economic</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>migration</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffering</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_1</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xenophobia</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                test_accuracy  test_f1  test_precision  test_recall  \\\n",
       "column                                                                \n",
       "benefits                 0.98      0.0             0.0          0.0   \n",
       "culture                  0.94      0.0             0.0          0.0   \n",
       "dehumanisation           0.97      0.0             0.0          0.0   \n",
       "economic                 0.98      0.0             0.0          0.0   \n",
       "health                   1.00      0.0             0.0          0.0   \n",
       "migration                0.92      0.0             0.0          0.0   \n",
       "others                   0.97      0.0             0.0          0.0   \n",
       "security                 0.89      0.0             0.0          0.0   \n",
       "suffering                0.96      0.0             0.0          0.0   \n",
       "task_1                   0.52      0.0             0.0          0.0   \n",
       "xenophobia               1.00      0.0             0.0          0.0   \n",
       "\n",
       "                train_accuracy  train_f1  train_precision  train_recall  \n",
       "column                                                                   \n",
       "benefits                  0.98       0.0              0.0           0.0  \n",
       "culture                   0.94       0.0              0.0           0.0  \n",
       "dehumanisation            0.97       0.0              0.0           0.0  \n",
       "economic                  0.98       0.0              0.0           0.0  \n",
       "health                    1.00       0.0              0.0           0.0  \n",
       "migration                 0.92       0.0              0.0           0.0  \n",
       "others                    0.97       0.0              0.0           0.0  \n",
       "security                  0.89       0.0              0.0           0.0  \n",
       "suffering                 0.96       0.0              0.0           0.0  \n",
       "task_1                    0.22       0.0              0.0           0.0  \n",
       "xenophobia                1.00       0.0              0.0           0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import sys\n",
    "sys.path.append('../../library/')\n",
    "import juan\n",
    "import utils\n",
    "\n",
    "importlib.reload(juan)\n",
    "importlib.reload(utils)\n",
    "clf = juan.BetoMTL(tokenizer, model)\n",
    "clf.to(device)\n",
    "\n",
    "# clf.entrenar(\n",
    "#     X,\n",
    "#     Y,\n",
    "#     epochs = 5,\n",
    "# #     freeze_encoder = True,\n",
    "# #     weight_decay = 1000,\n",
    "# #     class_weights = \"balanced\"\n",
    "# )\n",
    "\n",
    "# predictions = clf.predecir_proba(X)\n",
    "# # predictions = clf.predecir(X)\n",
    "# predictions[:5]\n",
    "\n",
    "results = utils.validate_MTL_juan(\n",
    "    X,\n",
    "    Y, \n",
    "#     X, Y,\n",
    "    clf, \n",
    "#     epochs = 5,\n",
    "#     freeze_encoder = True,\n",
    "    progress_bar = False\n",
    "#     weight_decay = 100,\n",
    "#     class_weights = \"balanced\"\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b44e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (100,)\n",
      "Y (100, 10)\n",
      "Cross-validation process started...\n",
      "*** fold 1 / 5\n",
      "    training model...\n",
      "Training model...\n",
      "    first step, train loss:0.1728\n",
      "    epoch: 1, train loss:0.1601, train accuracy: 0.9587\n",
      "    epoch: 2, train loss:0.1299, train accuracy: 0.9600\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "    Total runtime: 0.04 minutes\n",
      "*** fold 2 / 5\n",
      "    training model...\n",
      "Training model...\n",
      "    first step, train loss:0.0445\n",
      "    epoch: 1, train loss:0.1118, train accuracy: 0.9613\n",
      "    epoch: 2, train loss:0.0807, train accuracy: 0.9800\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "    Total runtime: 0.09 minutes\n",
      "*** fold 3 / 5\n",
      "    training model...\n",
      "Training model...\n",
      "    first step, train loss:0.0298\n",
      "    epoch: 1, train loss:0.0712, train accuracy: 0.9825\n",
      "    epoch: 2, train loss:0.0585, train accuracy: 0.9887\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "    Total runtime: 0.13 minutes\n",
      "*** fold 4 / 5\n",
      "    training model...\n",
      "Training model...\n",
      "    first step, train loss:0.0598\n",
      "    epoch: 1, train loss:0.0495, train accuracy: 0.9912\n",
      "    epoch: 2, train loss:0.0412, train accuracy: 0.9925\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "    Total runtime: 0.18 minutes\n",
      "*** fold 5 / 5\n",
      "    training model...\n",
      "Training model...\n",
      "    first step, train loss:0.0164\n",
      "    epoch: 1, train loss:0.0359, train accuracy: 0.9963\n",
      "    epoch: 2, train loss:0.0297, train accuracy: 0.9988\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "    Total runtime: 0.22 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>benefits</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>culture</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.713247</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dehumanisation</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economic</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>migration</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.9850</td>\n",
       "      <td>0.869231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.9875</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffering</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.9900</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_1</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.9200</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xenophobia</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                test_accuracy   test_f1  test_precision  test_recall  \\\n",
       "column                                                                 \n",
       "benefits                 0.99  0.200000             0.2         0.20   \n",
       "culture                  1.00  0.600000             0.6         0.60   \n",
       "dehumanisation           1.00  0.400000             0.4         0.40   \n",
       "economic                 0.98  0.000000             0.0         0.00   \n",
       "health                   1.00  0.000000             0.0         0.00   \n",
       "migration                0.97  0.600000             0.6         0.60   \n",
       "others                   0.99  0.400000             0.4         0.40   \n",
       "security                 0.98  0.800000             0.8         0.80   \n",
       "suffering                0.98  0.400000             0.4         0.40   \n",
       "task_1                   0.86  0.704762             0.8         0.65   \n",
       "xenophobia               1.00  0.000000             0.0         0.00   \n",
       "\n",
       "                train_accuracy  train_f1  train_precision  train_recall  \n",
       "column                                                                   \n",
       "benefits                0.9950  0.733333              0.8      0.700000  \n",
       "culture                 0.9775  0.713247              0.8      0.650000  \n",
       "dehumanisation          0.9950  0.900000              1.0      0.866667  \n",
       "economic                0.9925  0.533333              0.6      0.500000  \n",
       "health                  1.0000  0.000000              0.0      0.000000  \n",
       "migration               0.9850  0.869231              1.0      0.809524  \n",
       "others                  0.9925  0.826667              1.0      0.733333  \n",
       "security                0.9875  0.923077              1.0      0.888889  \n",
       "suffering               0.9900  0.800000              0.8      0.800000  \n",
       "task_1                  0.9200  0.933333              1.0      0.900000  \n",
       "xenophobia              1.0000  0.000000              0.0      0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = utils.validate_MTL_juan(\n",
    "    X,\n",
    "    Y, \n",
    "#     X, Y,\n",
    "    clf, \n",
    "#     epochs = 5,\n",
    "#     freeze_encoder = True,\n",
    "    progress_bar = False,\n",
    "    refresh=False\n",
    "#     weight_decay = 100,\n",
    "#     class_weights = \"balanced\"\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0544d582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdec166",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation process started...\n",
      "*** column 1 / 10 (xenophobia) - fold 1 / 5\n",
      "    training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d76ba8179e4fd8b043a8565fb92d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/maiso/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3053\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1146' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1146/1146 00:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.037300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer/checkpoint-500\n",
      "Configuration saved in test-trainer/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-1000\n",
      "Configuration saved in test-trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    generating predictions on the train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function BERTMultilingualUncased.predecir.<locals>.tokenize_function at 0x1513a310a820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0b446db4c246eba06716f12e8c44a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3053\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='478' max='382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [382/382 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    generating predictions on the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c66fe1835e8432d93079cbcd16d720b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 764\n",
      "  Batch size = 8\n",
      "Didn't find file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/added_tokens.json. We won't load it.\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/vocab.txt\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/tokenizer.json\n",
      "loading file None\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/special_tokens_map.json\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/tokenizer_config.json\n",
      "loading configuration file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading weights file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total runtime: 0.98 minutes\n",
      "*** column 1 / 10 (xenophobia) - fold 2 / 5\n",
      "    training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aac1d70f78048d999d6982499735879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/maiso/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3053\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1146' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1146/1146 00:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.031500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer/checkpoint-500\n",
      "Configuration saved in test-trainer/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-1000\n",
      "Configuration saved in test-trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    generating predictions on the train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f6560462c142d09efdffcd383e9271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3053\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='478' max='382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [382/382 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    generating predictions on the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372fefd836f74914870e8a64c2871768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 764\n",
      "  Batch size = 8\n",
      "Didn't find file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/added_tokens.json. We won't load it.\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/vocab.txt\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/tokenizer.json\n",
      "loading file None\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/special_tokens_map.json\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/tokenizer_config.json\n",
      "loading configuration file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading weights file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total runtime: 1.91 minutes\n",
      "*** column 1 / 10 (xenophobia) - fold 3 / 5\n",
      "    training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca5cb3eeb27474d88f3d5fe982ff47f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/maiso/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3054\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1146' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1146/1146 00:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.024900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer/checkpoint-500\n",
      "Configuration saved in test-trainer/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-1000\n",
      "Configuration saved in test-trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    generating predictions on the train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c013332d9c948e8baf8708699d1db06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3054\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='478' max='382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [382/382 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    generating predictions on the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2614e4bde5ce4e51bf6a3785c698b9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 763\n",
      "  Batch size = 8\n",
      "Didn't find file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/added_tokens.json. We won't load it.\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/vocab.txt\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/tokenizer.json\n",
      "loading file None\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/special_tokens_map.json\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/tokenizer_config.json\n",
      "loading configuration file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading weights file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total runtime: 2.83 minutes\n",
      "*** column 1 / 10 (xenophobia) - fold 4 / 5\n",
      "    training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51186a4037a048a3ac57e2a0c520177c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/maiso/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3054\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1146' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1146/1146 00:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.023200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer/checkpoint-500\n",
      "Configuration saved in test-trainer/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-1000\n",
      "Configuration saved in test-trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    generating predictions on the train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495a425272c441189a70d9f61f1a08c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3054\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='478' max='382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [382/382 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    generating predictions on the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf97ea7a22ed4ea9ad8b0b5ff07e9a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 763\n",
      "  Batch size = 8\n",
      "Didn't find file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/added_tokens.json. We won't load it.\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/vocab.txt\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/tokenizer.json\n",
      "loading file None\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/special_tokens_map.json\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/tokenizer_config.json\n",
      "loading configuration file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading weights file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total runtime: 3.76 minutes\n",
      "*** column 1 / 10 (xenophobia) - fold 5 / 5\n",
      "    training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835857cc378841b79b1537e8b2b469c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/maiso/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3054\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1146' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1146/1146 00:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.032400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer/checkpoint-500\n",
      "Configuration saved in test-trainer/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-1000\n",
      "Configuration saved in test-trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    generating predictions on the train set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e27b2a15c84e898da69c5efbfc85d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3054\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='478' max='382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [382/382 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    generating predictions on the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5259ca40bde94061818ddc82b68afa0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 763\n",
      "  Batch size = 8\n",
      "Didn't find file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/added_tokens.json. We won't load it.\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/vocab.txt\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/tokenizer.json\n",
      "loading file None\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/special_tokens_map.json\n",
      "loading file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/tokenizer_config.json\n",
      "loading configuration file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "loading weights file ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Total runtime: 4.69 minutes\n",
      "*** column 2 / 10 (suffering) - fold 1 / 5\n",
      "    training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d58729e8384e8dac2ff401339a201b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence. If sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/maiso/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3053\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1146\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1146 00:39 < 00:05, 25.49 it/s, Epoch 2.62/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.090400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test-trainer/checkpoint-500\n",
      "Configuration saved in test-trainer/checkpoint-500/config.json\n",
      "Model weights saved in test-trainer/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to test-trainer/checkpoint-1000\n",
      "Configuration saved in test-trainer/checkpoint-1000/config.json\n",
      "Model weights saved in test-trainer/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in test-trainer/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in test-trainer/checkpoint-1000/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "logging.getLogger(\"pytorch_pretrained_bert.tokenization\").setLevel(logging.ERROR)\n",
    "sys.path.append('../../library/')\n",
    "from maiso.models import BERTMultilingualUncased\n",
    "from maiso.task import get_texts,get_y\n",
    "from utils import validate\n",
    "results =   validate(\n",
    "                BERTMultilingualUncased.get_X(get_texts()), \n",
    "                get_y(), \n",
    "                BERTMultilingualUncased()) \n",
    "\n",
    "results.to_csv(\"../../results/task_2/data/BERT_multilingual_uncased.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e69a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../../library/')\n",
    "# from maiso.models import BERTMultilingualUncased\n",
    "# from maiso.task import get_texts,get_y\n",
    "\n",
    "\n",
    "# model = BERTMultilingualUncased()\n",
    "# # model.entrenar(get_text(),)\n",
    "\n",
    "# from maiso.task import get_texts,get_y\n",
    "# X = BERTMultilingualUncased.get_X(get_texts()[:10])\n",
    "# y = get_y()\n",
    "# model.entrenar(X, y.iloc[:10,0])\n",
    "# model.predecir(BERTMultilingualUncased.get_X(get_texts()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../../library/')\n",
    "# class BERTMultilingualUncased(object):\n",
    "#     def __init__(self):\n",
    "#         self.trained=False\n",
    "\n",
    "#     def entrenar(self, X, y):\n",
    "#         model_name = '../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/'\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        \n",
    "#         assert all([type(elem)==str for elem in X])\n",
    "#         data = {'sentence':[text for text in X],\n",
    "#                 'label': [label for label in y]\n",
    "#                }\n",
    "#         train_data = Dataset.from_dict(data)\n",
    "#         def tokenize_function(example):\n",
    "#             return self.tokenizer(example[\"sentence\"], truncation=True)\n",
    "        \n",
    "#         tokenized_dataset = train_data.map(tokenize_function, batched=True)\n",
    "#         from transformers import Trainer\n",
    "#         from transformers import TrainingArguments\n",
    "\n",
    "#         training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "\n",
    "#         data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "#         training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model,\n",
    "#             training_args,\n",
    "#             train_dataset=tokenized_dataset,\n",
    "#             data_collator=data_collator,\n",
    "#             tokenizer=self.tokenizer,\n",
    "#         )\n",
    "#         output = trainer.train()\n",
    "#         self.trainer = trainer\n",
    "#         self.model = model\n",
    "#         self.trained=True\n",
    "#         return output\n",
    "#     def get_X(text_list):\n",
    "#         return text_list\n",
    "#     def predecir(self, X):\n",
    "#         assert self.trained\n",
    "#         data = {'sentence':[text for text in X]\n",
    "#                }\n",
    "#         dataset = Dataset.from_dict(data)\n",
    "\n",
    "#         def tokenize_function(example):\n",
    "#             return self.tokenizer(example[\"sentence\"], truncation=True)\n",
    "        \n",
    "#         tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "#         predictions = self.trainer.predict(tokenized_dataset)\n",
    "\n",
    "        \n",
    "#         return np.argmax(predictions.predictions, axis=-1)\n",
    "# from maiso.task import get_texts,get_y\n",
    "# get_texts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df882677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "# import numpy as np \n",
    "# class BERTMultilingualUncased(object):\n",
    "#     def __init__(self):\n",
    "#         self.trained=False\n",
    "\n",
    "#     def entrenar(self, X, y):\n",
    "#         model_name = '../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/'\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        \n",
    "#         assert all([type(elem)==str for elem in X])\n",
    "#         data = {'sentence':[text for text in X],\n",
    "#                 'label': [label for label in y]\n",
    "#                }\n",
    "#         train_data = Dataset.from_dict(data)\n",
    "#         def tokenize_function(example):\n",
    "#             return self.tokenizer(example[\"sentence\"], truncation=True)\n",
    "        \n",
    "#         tokenized_dataset = train_data.map(tokenize_function, batched=True)\n",
    "#         from transformers import Trainer\n",
    "#         from transformers import TrainingArguments\n",
    "\n",
    "#         training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "\n",
    "#         data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "#         training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "#         trainer = Trainer(\n",
    "#             model,\n",
    "#             training_args,\n",
    "#             train_dataset=tokenized_dataset,\n",
    "#             data_collator=data_collator,\n",
    "#             tokenizer=self.tokenizer,\n",
    "#         )\n",
    "#         output = trainer.train()\n",
    "#         self.trainer = trainer\n",
    "#         self.model = model\n",
    "#         self.trained=True\n",
    "#         return output\n",
    "#     def predecir(self, X):\n",
    "#         assert self.trained\n",
    "#         data = {'sentence':[text for text in X]\n",
    "#                }\n",
    "#         dataset = Dataset.from_dict(data)\n",
    "\n",
    "#         def tokenize_function(example):\n",
    "#             return self.tokenizer(example[\"sentence\"], truncation=True)\n",
    "        \n",
    "#         tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "#         predictions = self.trainer.predict(tokenized_dataset)\n",
    "\n",
    "        \n",
    "#         return np.argmax(predictions.predictions, axis=-1)\n",
    "# model = BERTMultilingualUncased()\n",
    "# # model.entrenar(get_text(),)\n",
    "\n",
    "# from maiso.task import get_texts,get_y\n",
    "# y = get_y()\n",
    "# model.entrenar(get_texts()[:10], y.iloc[:10,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261063d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd4e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predecir(get_texts()[:50])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c147936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1215432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.trainer.predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a44101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from maiso.task import get_texts,get_y\n",
    "# y = get_y()\n",
    "# model.entrenar(get_texts()[:200], y.iloc[:200,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af56c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# data_path = '/lustre06/project/6001735/detests_2022/data/train.csv'\n",
    "# raw_datasets = load_dataset('csv', data_files=data_path)\n",
    "\n",
    "\n",
    "# raw_datasets['train'] = raw_datasets['train'].remove_columns(['comment_id',\n",
    "#                                                               'sentence_pos',\n",
    "#                                                               'reply_to',\n",
    "#                                                               'racial_target',\n",
    "#                                                               'other_target',\n",
    "#                                                               'implicit',\n",
    "#                                                               'stereotype',\n",
    "#                                                              ])\n",
    "\n",
    "\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "# model_name = '../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# def tokenize_function(example):\n",
    "#     return tokenizer(example[\"sentence\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "\n",
    "# raw_datasets['train']\n",
    "\n",
    "# tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "# tokenized_datasets\n",
    "\n",
    "# tokenized_datasets['train'] = tokenized_datasets['train'].rename_column('suffering', 'label')\n",
    "# tokenized_datasets\n",
    "\n",
    "# columns_to_remove = [feature for feature in tokenized_datasets['train'].features if not feature in \n",
    "#                      ['sentence','input_ids','token_type_ids','attention_mask','label']]\n",
    "# tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns(columns_to_remove)\n",
    "# tokenized_datasets['train']\n",
    "\n",
    "# from transformers import Trainer\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "# #     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716240e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_datasets['train'].rename_column('suffering','label')\n",
    "\n",
    "\n",
    "# columns_to_remove = [feature for feature in raw_datasets['train'].features if not feature in \n",
    "#                      ['input_ids','token_type_ids','attention_mask','label']]\n",
    "\n",
    "# raw_datasets['train'] = raw_datasets['train'].remove_columns(columns_to_remove)\n",
    "# raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc987e9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "# from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "# model_name = '../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def tokenize_function(example):\n",
    "#     return tokenizer(example[\"sentence\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "# # columns_to_remove = [feature for feature in tokenized_datasets['train'].features if not feature in ['input_ids','token_type_ids','attention_mask','suffering']]\n",
    "# # tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns(columns_to_remove)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# raw_datasets['train'].rename_column('suffering','label')\n",
    "\n",
    "\n",
    "# columns_to_remove = [feature for feature in tokenized_datasets['train'].features if not feature in \n",
    "#                      ['input_ids','token_type_ids','attention_mask','label']]\n",
    "\n",
    "# raw_datasets['train'] = raw_datasets['train'].remove_columns(columns_to_remove)\n",
    "\n",
    "# tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "# from transformers import Trainer\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "# #     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce4f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.drop_index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(data_path)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de703f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100b2318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f795b430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c9bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "# from transformers import AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# model_name = '../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# corpus = ['Some sentence.', 'Some other sentence.', 'A third sentence.']\n",
    "\n",
    "# outputs = model(**tokenizer(corpus, padding=True, truncation=True,return_tensors='pt'))\n",
    "\n",
    "\n",
    "\n",
    "# predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "# print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import ClassLabel\n",
    "# ClassLabel(num_classes=2, names=['NEGATIVE', 'POSITIVE',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b39c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# data_path = '/lustre06/project/6001735/detests_2022/data/train.csv'\n",
    "# raw_dataset = load_dataset('csv', data_files=data_path)\n",
    "# # raw_dataset['train']['label'] = raw_dataset['train']['xenophobia']\n",
    "# raw_dataset['train'] = raw_dataset['train'].add_column('label', ['POSITIVE' if elem==1 else 'NEGATIVE' for elem in raw_dataset['train']['xenophobia']])\n",
    "# dataset = raw_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb988f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa46813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../../library/')\n",
    "# from maiso.task import get_y, get_texts\n",
    "\n",
    "# corpus = get_texts()\n",
    "# # corpus[:2]\n",
    "# # tokenized_datasets = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0480278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasetsdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# data_path = '/lustre06/project/6001735/detests_2022/data/train.csv'\n",
    "# raw_datasets = load_dataset('csv', data_files=data_path)\n",
    "# raw_datasets['train'] = raw_datasets['train'].add_column('label', ['POSITIVE' if elem==1 else 'NEGATIVE' for elem in raw_datasets['train']['xenophobia']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c105b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# data_path = '/lustre06/project/6001735/detests_2022/data/train.csv'\n",
    "# raw_datasets = load_dataset('csv', data_files=data_path)\n",
    "# raw_datasets['train'] = raw_datasets['train'].add_column('label', ['POSITIVE' if elem==1 else 'NEGATIVE' for elem in raw_datasets['train']['xenophobia']])\n",
    "\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "# # raw_datasets = #load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# def tokenize_function(example):\n",
    "#     return tokenizer(example[\"sentence\"], truncation=True)\n",
    "\n",
    "\n",
    "# tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "# from transformers import Trainer\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "# #     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('../../library/')\n",
    "# from transformers import AdamW\n",
    "# from maiso.task import get_y, get_texts\n",
    "\n",
    "# batch = tokenizer(get_texts(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# # This is new\n",
    "# batch[\"labels\"] = torch.tensor(get_y().loc[:,\"migration\"].values)\n",
    "\n",
    "# optimizer = AdamW(model.parameters())\n",
    "# loss = model(**batch).loss\n",
    "# loss.backward()\n",
    "# optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a15f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../../../../../../home/maiso/.cache/huggingface/bert-base-multilingual-uncased/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175890a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

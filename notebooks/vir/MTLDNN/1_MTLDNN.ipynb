{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03524b0-ea98-4854-8f2c-2883f548f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data import load_data\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,Callback\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56f4dbe-ec08-4c64-8ea9-c561c3f000ea",
   "metadata": {},
   "source": [
    "## Multi task learning DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13375cb8-1c2b-4a63-87ba-d76af318fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s):\n",
    "    K.clear_session()\n",
    "    \n",
    "    # Seed value\n",
    "    # Apparently you may use different seed values at each stage\n",
    "    seed_value= s\n",
    "\n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    import os\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    import random\n",
    "    random.seed(seed_value)\n",
    "\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    import numpy as np\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    # 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "    import tensorflow as tf\n",
    "    tf.random.set_seed(seed_value)\n",
    "    # for later versions: \n",
    "    # tf.compat.v1.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2819bf5c-5433-4b4e-b66e-3df9017b2110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsabando/virtualenvs/tf_gpu/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "datapath =\"../../data/task_2.csv\"\n",
    "train_folds = load_data(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73b00c15-6214-4108-9699-ed2aff9431d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------\n",
    "# Algunas constantes\n",
    "#--------------------------------------------------------\n",
    "\n",
    "#dropout\n",
    "prob_h1 = 0.25\n",
    "prob_h2 = 0.15\n",
    "prob_h3 = 0.1\n",
    "\n",
    "#batch normalization\n",
    "momentum_batch_norm = 0.9\n",
    "\n",
    "#adam optimizador\n",
    "l_rate = 0.0001\n",
    "\n",
    "#optimizador\n",
    "adam_opt = Adam(l_rate)\n",
    "\n",
    "# activacion capas internas\n",
    "act = 'relu'\n",
    "\n",
    "#early stopping\n",
    "min_delta_val = 0.0001\n",
    "patience_val = 500\n",
    "\n",
    "# n_epochs = np.iinfo(np.int32).max\n",
    "n_epochs = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f99cffa8-5c1c-4204-b679-f139832fd912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specific_layers(x, n):\n",
    "    y1 = Dropout(prob_h3)(Dense(n, activation=act)(x))\n",
    "    y2 = Dropout(prob_h3)(Dense(n, activation=act)(x))\n",
    "    y3 = Dropout(prob_h3)(Dense(n, activation=act)(x))\n",
    "    y4 = Dropout(prob_h3)(Dense(n, activation=act)(x))\n",
    "    y5 = Dropout(prob_h3)(Dense(n, activation=act)(x))\n",
    "    y6 = Dropout(prob_h3)(Dense(n, activation=act)(x))\n",
    "    y7 = Dropout(prob_h3)(Dense(n, activation=act)(x))\n",
    "    y8 = Dropout(prob_h3)(Dense(n, activation=act)(x))   \n",
    "    y9 = Dropout(prob_h3)(Dense(n, activation=act)(x))\n",
    "    y10 = Dropout(prob_h3)(Dense(n, activation=act)(x))\n",
    "  \n",
    "    return y1,y2,y3,y4,y5,y6,y7,y8,y9,y10\n",
    "\n",
    "def build_model(ni, n0, n1, n2, n3, ln, act):\n",
    "\n",
    "    # ENTRADA\n",
    "    model_input = Input(shape=(ni,))\n",
    "    \n",
    "    # CAPAS COMUNES A TODAS LOS TARGETS\n",
    "    x = Dense(n0, activation=act)(model_input)\n",
    "    # x = BatchNormalization(momentum=momentum_batch_norm)(x)\n",
    "    x = Dropout(prob_h1)(x)\n",
    "    x = Dense(n1, activation=act)(x)\n",
    "    # x = BatchNormalization(momentum=momentum_batch_norm)(x)\n",
    "    x = Dropout(prob_h2)(x)\n",
    "    x = Dense(n2, activation=act)(x)\n",
    "    # x = BatchNormalization(momentum=momentum_batch_norm)(x)\n",
    "    x = Dropout(prob_h3)(x)\n",
    "    \n",
    "    # CAPAS ESPECIFICAS A CADA TARGET\n",
    "    y1,y2,y3,y4,y5,y6,y7,y8,y9,y10 = specific_layers(x, n3)\n",
    "    \n",
    "\n",
    "    # TARGETS 1 a 15\n",
    "    y1 = Dense(units = 1,activation= 'sigmoid', name = 'output_1')(y1)\n",
    "    y2 = Dense(units = 1,activation= 'sigmoid', name = 'output_2')(y2)\n",
    "    y3 = Dense(units = 1,activation= 'sigmoid', name = 'output_3')(y3)\n",
    "    y4 = Dense(units = 1,activation= 'sigmoid', name = 'output_4')(y4)\n",
    "    y5 = Dense(units = 1,activation= 'sigmoid', name = 'output_5')(y5)\n",
    "    y6 = Dense(units = 1,activation= 'sigmoid', name = 'output_6')(y6)\n",
    "    y7 = Dense(units = 1,activation= 'sigmoid', name = 'output_7')(y7)\n",
    "    y8 = Dense(units = 1,activation= 'sigmoid', name = 'output_8')(y8)    \n",
    "    y9 = Dense(units = 1,activation= 'sigmoid', name = 'output_9')(y9)\n",
    "    y10 =Dense(units = 1,activation= 'sigmoid', name = 'output_10')(y10)\n",
    "\n",
    "    # MODELO FINAL\n",
    "    model = Model(inputs=model_input, outputs=[y1,y2,y3,y4,y5,y6,y7,y8,y9,y10])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d1d5a3b-2ee9-4d83-9f68-2a66ed613a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_MTL(x_train, x_val, y_train, y_val, lb = 0.005, arq = (100,50,10,5), w = None):\n",
    "    \n",
    "    n_inputs = x_train.shape[1]\n",
    "    # regularizacion l2 \n",
    "    reg = l2(lb)\n",
    "    #optimizador\n",
    "    adam_opt = Adam(l_rate)\n",
    "    #early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss',\n",
    "                               min_delta=min_delta_val,\n",
    "                               patience=patience_val,\n",
    "                               verbose=0, \n",
    "                               mode='min',\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "    model = build_model(n_inputs, arq[0], arq[1], arq[2], arq[3], reg, act)\n",
    "\n",
    "    #compilar el modelo\n",
    "    model.compile(loss=BinaryCrossentropy(), optimizer=adam_opt, metrics = [tf.keras.metrics.Precision(),\n",
    "                                                                            tf.keras.metrics.Recall(),\n",
    "                                                                            tf.keras.metrics.Accuracy()])\n",
    "    \n",
    "    #entrenar el modelo\n",
    "    learning_data = model.fit(x=x_train,\n",
    "                              y=y_train,\n",
    "                              epochs=n_epochs,\n",
    "                              validation_data=(x_val,y_val),\n",
    "                              callbacks=[early_stop],\n",
    "                              class_weight=w,\n",
    "                              verbose=2)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e29ac64-2db4-4129-b200-69301a60b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------\n",
    "pesos = {\n",
    "    '97':{0:1,1:3.47},\n",
    "    '98':{0:1,1:1.90},\n",
    "    '100':{0:1,1:1.56},\n",
    "    '102':{0:1,1:3.31},\n",
    "    '1535':{0:1,1:5.09},\n",
    "    '1537':{0:1,1:5.11},\n",
    "    '1538':{0:1,1:2.81},\n",
    "    'Extra':{0:1,1:1.10},\n",
    "    'Overall':{0:1.40,1:1},\n",
    "    'Bal': {0:1.1,1:1, -1:0} \n",
    "}\n",
    "\n",
    "params_dict = {'a':{'lb':0.005,'arq':(100,50,10,5),'w': None}}\n",
    "seed_lst = [24]\n",
    "\n",
    "#------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "953da120-a216-4ba2-80e7-3e7790f116c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n",
      "Train on 3053 samples, validate on 764 samples\n",
      "3053/3053 - 92s - loss: 6.8781 - output_1_loss: 0.7015 - output_2_loss: 0.6774 - output_3_loss: 0.6807 - output_4_loss: 0.6859 - output_5_loss: 0.6779 - output_6_loss: 0.6764 - output_7_loss: 0.6992 - output_8_loss: 0.6897 - output_9_loss: 0.6919 - output_10_loss: 0.6971 - output_1_precision: 0.0040 - output_1_recall: 0.6429 - output_1_accuracy: 0.0000e+00 - output_2_precision: 0.0000e+00 - output_2_recall: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - output_3_precision: 0.0000e+00 - output_3_recall: 0.0000e+00 - output_3_accuracy: 0.0000e+00 - output_4_precision: 0.0732 - output_4_recall: 0.0931 - output_4_accuracy: 0.0000e+00 - output_5_precision: 0.0578 - output_5_recall: 0.1189 - output_5_accuracy: 0.0000e+00 - output_6_precision: 0.0582 - output_6_recall: 0.1965 - output_6_accuracy: 0.0000e+00 - output_7_precision: 0.0042 - output_7_recall: 0.4667 - output_7_accuracy: 0.0000e+00 - output_8_precision: 0.0345 - output_8_recall: 0.0284 - output_8_accuracy: 0.0000e+00 - output_9_precision: 0.0131 - output_9_recall: 0.2885 - output_9_accuracy: 0.0000e+00 - output_10_precision: 0.0170 - output_10_recall: 0.7600 - output_10_accuracy: 0.0000e+00 - val_loss: 6.7764 - val_output_1_loss: 0.6957 - val_output_2_loss: 0.6588 - val_output_3_loss: 0.6661 - val_output_4_loss: 0.6818 - val_output_5_loss: 0.6602 - val_output_6_loss: 0.6476 - val_output_7_loss: 0.6913 - val_output_8_loss: 0.6850 - val_output_9_loss: 0.6918 - val_output_10_loss: 0.6980 - val_output_1_precision: 0.0038 - val_output_1_recall: 1.0000 - val_output_1_accuracy: 0.0000e+00 - val_output_2_precision: 0.0000e+00 - val_output_2_recall: 0.0000e+00 - val_output_2_accuracy: 0.0000e+00 - val_output_3_precision: 0.0000e+00 - val_output_3_recall: 0.0000e+00 - val_output_3_accuracy: 0.0000e+00 - val_output_4_precision: 0.0000e+00 - val_output_4_recall: 0.0000e+00 - val_output_4_accuracy: 0.0000e+00 - val_output_5_precision: 0.0000e+00 - val_output_5_recall: 0.0000e+00 - val_output_5_accuracy: 0.0000e+00 - val_output_6_precision: 0.0000e+00 - val_output_6_recall: 0.0000e+00 - val_output_6_accuracy: 0.0000e+00 - val_output_7_precision: 0.0000e+00 - val_output_7_recall: 0.0000e+00 - val_output_7_accuracy: 0.0000e+00 - val_output_8_precision: 0.0000e+00 - val_output_8_recall: 0.0000e+00 - val_output_8_accuracy: 0.0000e+00 - val_output_9_precision: 0.0271 - val_output_9_recall: 0.4615 - val_output_9_accuracy: 0.0000e+00 - val_output_10_precision: 0.0219 - val_output_10_recall: 0.8824 - val_output_10_accuracy: 0.0000e+00\n",
      "764/764 [==============================] - 0s 239us/sample - loss: 6.7764 - output_1_loss: 0.6957 - output_2_loss: 0.6588 - output_3_loss: 0.6661 - output_4_loss: 0.6818 - output_5_loss: 0.6602 - output_6_loss: 0.6476 - output_7_loss: 0.6913 - output_8_loss: 0.6850 - output_9_loss: 0.6918 - output_10_loss: 0.6980 - output_1_precision: 0.0038 - output_1_recall: 1.0000 - output_1_accuracy: 0.0000e+00 - output_2_precision: 0.0000e+00 - output_2_recall: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - output_3_precision: 0.0000e+00 - output_3_recall: 0.0000e+00 - output_3_accuracy: 0.0000e+00 - output_4_precision: 0.0000e+00 - output_4_recall: 0.0000e+00 - output_4_accuracy: 0.0000e+00 - output_5_precision: 0.0000e+00 - output_5_recall: 0.0000e+00 - output_5_accuracy: 0.0000e+00 - output_6_precision: 0.0000e+00 - output_6_recall: 0.0000e+00 - output_6_accuracy: 0.0000e+00 - output_7_precision: 0.0000e+00 - output_7_recall: 0.0000e+00 - output_7_accuracy: 0.0000e+00 - output_8_precision: 0.0000e+00 - output_8_recall: 0.0000e+00 - output_8_accuracy: 0.0000e+00 - output_9_precision: 0.0271 - output_9_recall: 0.4615 - output_9_accuracy: 0.0000e+00 - output_10_precision: 0.0219 - output_10_recall: 0.8824 - output_10_accuracy: 0.0000e+00              \n",
      "hola\n",
      "Train on 3053 samples, validate on 764 samples\n",
      "3053/3053 - 10s - loss: 6.8778 - output_1_loss: 0.7018 - output_2_loss: 0.6766 - output_3_loss: 0.6805 - output_4_loss: 0.6862 - output_5_loss: 0.6774 - output_6_loss: 0.6764 - output_7_loss: 0.6992 - output_8_loss: 0.6895 - output_9_loss: 0.6924 - output_10_loss: 0.6973 - output_1_precision: 0.0039 - output_1_recall: 0.9000 - output_1_accuracy: 0.0000e+00 - output_2_precision: 0.0000e+00 - output_2_recall: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - output_3_precision: 0.0189 - output_3_recall: 0.0208 - output_3_accuracy: 0.0000e+00 - output_4_precision: 0.0678 - output_4_recall: 0.0878 - output_4_accuracy: 0.0000e+00 - output_5_precision: 0.0544 - output_5_recall: 0.1060 - output_5_accuracy: 0.0000e+00 - output_6_precision: 0.0422 - output_6_recall: 0.1534 - output_6_accuracy: 0.0000e+00 - output_7_precision: 0.0042 - output_7_recall: 0.5385 - output_7_accuracy: 0.0000e+00 - output_8_precision: 0.0645 - output_8_recall: 0.0488 - output_8_accuracy: 0.0000e+00 - output_9_precision: 0.0161 - output_9_recall: 0.4082 - output_9_accuracy: 0.0000e+00 - output_10_precision: 0.0188 - output_10_recall: 0.7963 - output_10_accuracy: 0.0000e+00 - val_loss: 6.7761 - val_output_1_loss: 0.6954 - val_output_2_loss: 0.6584 - val_output_3_loss: 0.6657 - val_output_4_loss: 0.6825 - val_output_5_loss: 0.6592 - val_output_6_loss: 0.6490 - val_output_7_loss: 0.6910 - val_output_8_loss: 0.6849 - val_output_9_loss: 0.6922 - val_output_10_loss: 0.6978 - val_output_1_precision: 0.0112 - val_output_1_recall: 1.0000 - val_output_1_accuracy: 0.0000e+00 - val_output_2_precision: 0.0000e+00 - val_output_2_recall: 0.0000e+00 - val_output_2_accuracy: 0.0000e+00 - val_output_3_precision: 0.0000e+00 - val_output_3_recall: 0.0000e+00 - val_output_3_accuracy: 0.0000e+00 - val_output_4_precision: 0.0000e+00 - val_output_4_recall: 0.0000e+00 - val_output_4_accuracy: 0.0000e+00 - val_output_5_precision: 0.0000e+00 - val_output_5_recall: 0.0000e+00 - val_output_5_accuracy: 0.0000e+00 - val_output_6_precision: 0.0000e+00 - val_output_6_recall: 0.0000e+00 - val_output_6_accuracy: 0.0000e+00 - val_output_7_precision: 0.0000e+00 - val_output_7_recall: 0.0000e+00 - val_output_7_accuracy: 0.0000e+00 - val_output_8_precision: 0.0000e+00 - val_output_8_recall: 0.0000e+00 - val_output_8_accuracy: 0.0000e+00 - val_output_9_precision: 0.0201 - val_output_9_recall: 0.3125 - val_output_9_accuracy: 0.0000e+00 - val_output_10_precision: 0.0176 - val_output_10_recall: 0.9231 - val_output_10_accuracy: 0.0000e+00\n",
      "764/764 [==============================] - 0s 237us/sample - loss: 6.7761 - output_1_loss: 0.6954 - output_2_loss: 0.6584 - output_3_loss: 0.6657 - output_4_loss: 0.6825 - output_5_loss: 0.6592 - output_6_loss: 0.6490 - output_7_loss: 0.6910 - output_8_loss: 0.6849 - output_9_loss: 0.6922 - output_10_loss: 0.6978 - output_1_precision: 0.0112 - output_1_recall: 1.0000 - output_1_accuracy: 0.0000e+00 - output_2_precision: 0.0000e+00 - output_2_recall: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - output_3_precision: 0.0000e+00 - output_3_recall: 0.0000e+00 - output_3_accuracy: 0.0000e+00 - output_4_precision: 0.0000e+00 - output_4_recall: 0.0000e+00 - output_4_accuracy: 0.0000e+00 - output_5_precision: 0.0000e+00 - output_5_recall: 0.0000e+00 - output_5_accuracy: 0.0000e+00 - output_6_precision: 0.0000e+00 - output_6_recall: 0.0000e+00 - output_6_accuracy: 0.0000e+00 - output_7_precision: 0.0000e+00 - output_7_recall: 0.0000e+00 - output_7_accuracy: 0.0000e+00 - output_8_precision: 0.0000e+00 - output_8_recall: 0.0000e+00 - output_8_accuracy: 0.0000e+00 - output_9_precision: 0.0201 - output_9_recall: 0.3125 - output_9_accuracy: 0.0000e+00 - output_10_precision: 0.0176 - output_10_recall: 0.9231 - output_10_accuracy: 0.0000e+00              \n",
      "hola\n",
      "Train on 3054 samples, validate on 763 samples\n",
      "3054/3054 - 10s - loss: 6.8780 - output_1_loss: 0.7015 - output_2_loss: 0.6770 - output_3_loss: 0.6801 - output_4_loss: 0.6862 - output_5_loss: 0.6781 - output_6_loss: 0.6762 - output_7_loss: 0.6989 - output_8_loss: 0.6898 - output_9_loss: 0.6923 - output_10_loss: 0.6974 - output_1_precision: 0.0039 - output_1_recall: 0.9000 - output_1_accuracy: 0.0000e+00 - output_2_precision: 0.0000e+00 - output_2_recall: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - output_3_precision: 0.0164 - output_3_recall: 0.0270 - output_3_accuracy: 0.0000e+00 - output_4_precision: 0.0930 - output_4_recall: 0.1236 - output_4_accuracy: 0.0000e+00 - output_5_precision: 0.0383 - output_5_recall: 0.0714 - output_5_accuracy: 0.0000e+00 - output_6_precision: 0.0564 - output_6_recall: 0.1939 - output_6_accuracy: 0.0000e+00 - output_7_precision: 0.0031 - output_7_recall: 0.3571 - output_7_accuracy: 0.0000e+00 - output_8_precision: 0.0455 - output_8_recall: 0.0386 - output_8_accuracy: 0.0000e+00 - output_9_precision: 0.0185 - output_9_recall: 0.4107 - output_9_accuracy: 0.0000e+00 - output_10_precision: 0.0193 - output_10_recall: 0.7586 - output_10_accuracy: 0.0000e+00 - val_loss: 6.7780 - val_output_1_loss: 0.6952 - val_output_2_loss: 0.6587 - val_output_3_loss: 0.6667 - val_output_4_loss: 0.6813 - val_output_5_loss: 0.6596 - val_output_6_loss: 0.6494 - val_output_7_loss: 0.6911 - val_output_8_loss: 0.6857 - val_output_9_loss: 0.6920 - val_output_10_loss: 0.6983 - val_output_1_precision: 0.0081 - val_output_1_recall: 0.6667 - val_output_1_accuracy: 0.0000e+00 - val_output_2_precision: 0.0000e+00 - val_output_2_recall: 0.0000e+00 - val_output_2_accuracy: 0.0000e+00 - val_output_3_precision: 0.0000e+00 - val_output_3_recall: 0.0000e+00 - val_output_3_accuracy: 0.0000e+00 - val_output_4_precision: 0.0000e+00 - val_output_4_recall: 0.0000e+00 - val_output_4_accuracy: 0.0000e+00 - val_output_5_precision: 0.0000e+00 - val_output_5_recall: 0.0000e+00 - val_output_5_accuracy: 0.0000e+00 - val_output_6_precision: 0.0000e+00 - val_output_6_recall: 0.0000e+00 - val_output_6_accuracy: 0.0000e+00 - val_output_7_precision: 0.0000e+00 - val_output_7_recall: 0.0000e+00 - val_output_7_accuracy: 0.0000e+00 - val_output_8_precision: 0.0000e+00 - val_output_8_recall: 0.0000e+00 - val_output_8_accuracy: 0.0000e+00 - val_output_9_precision: 0.0124 - val_output_9_recall: 0.3333 - val_output_9_accuracy: 0.0000e+00 - val_output_10_precision: 0.0100 - val_output_10_recall: 0.7778 - val_output_10_accuracy: 0.0000e+00\n",
      "763/763 [==============================] - 0s 235us/sample - loss: 6.7780 - output_1_loss: 0.6952 - output_2_loss: 0.6587 - output_3_loss: 0.6667 - output_4_loss: 0.6813 - output_5_loss: 0.6596 - output_6_loss: 0.6494 - output_7_loss: 0.6911 - output_8_loss: 0.6857 - output_9_loss: 0.6920 - output_10_loss: 0.6983 - output_1_precision: 0.0081 - output_1_recall: 0.6667 - output_1_accuracy: 0.0000e+00 - output_2_precision: 0.0000e+00 - output_2_recall: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - output_3_precision: 0.0000e+00 - output_3_recall: 0.0000e+00 - output_3_accuracy: 0.0000e+00 - output_4_precision: 0.0000e+00 - output_4_recall: 0.0000e+00 - output_4_accuracy: 0.0000e+00 - output_5_precision: 0.0000e+00 - output_5_recall: 0.0000e+00 - output_5_accuracy: 0.0000e+00 - output_6_precision: 0.0000e+00 - output_6_recall: 0.0000e+00 - output_6_accuracy: 0.0000e+00 - output_7_precision: 0.0000e+00 - output_7_recall: 0.0000e+00 - output_7_accuracy: 0.0000e+00 - output_8_precision: 0.0000e+00 - output_8_recall: 0.0000e+00 - output_8_accuracy: 0.0000e+00 - output_9_precision: 0.0124 - output_9_recall: 0.3333 - output_9_accuracy: 0.0000e+00 - output_10_precision: 0.0100 - output_10_recall: 0.7778 - output_10_accuracy: 0.0000e+00              \n",
      "hola\n",
      "Train on 3054 samples, validate on 763 samples\n",
      "3054/3054 - 10s - loss: 6.8784 - output_1_loss: 0.7018 - output_2_loss: 0.6768 - output_3_loss: 0.6804 - output_4_loss: 0.6863 - output_5_loss: 0.6777 - output_6_loss: 0.6765 - output_7_loss: 0.6993 - output_8_loss: 0.6896 - output_9_loss: 0.6922 - output_10_loss: 0.6973 - output_1_precision: 0.0049 - output_1_recall: 0.7333 - output_1_accuracy: 0.0000e+00 - output_2_precision: 0.0000e+00 - output_2_recall: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - output_3_precision: 0.0000e+00 - output_3_recall: 0.0000e+00 - output_3_accuracy: 0.0000e+00 - output_4_precision: 0.0836 - output_4_recall: 0.1103 - output_4_accuracy: 0.0000e+00 - output_5_precision: 0.0446 - output_5_recall: 0.0774 - output_5_accuracy: 0.0000e+00 - output_6_precision: 0.0433 - output_6_recall: 0.1625 - output_6_accuracy: 0.0000e+00 - output_7_precision: 0.0061 - output_7_recall: 0.6667 - output_7_accuracy: 0.0000e+00 - output_8_precision: 0.0530 - output_8_recall: 0.0404 - output_8_accuracy: 0.0000e+00 - output_9_precision: 0.0174 - output_9_recall: 0.3889 - output_9_accuracy: 0.0000e+00 - output_10_precision: 0.0159 - output_10_recall: 0.7347 - output_10_accuracy: 0.0000e+00 - val_loss: 6.7778 - val_output_1_loss: 0.6951 - val_output_2_loss: 0.6588 - val_output_3_loss: 0.6663 - val_output_4_loss: 0.6821 - val_output_5_loss: 0.6594 - val_output_6_loss: 0.6501 - val_output_7_loss: 0.6909 - val_output_8_loss: 0.6852 - val_output_9_loss: 0.6921 - val_output_10_loss: 0.6976 - val_output_1_precision: 0.0020 - val_output_1_recall: 1.0000 - val_output_1_accuracy: 0.0000e+00 - val_output_2_precision: 0.0000e+00 - val_output_2_recall: 0.0000e+00 - val_output_2_accuracy: 0.0000e+00 - val_output_3_precision: 0.0000e+00 - val_output_3_recall: 0.0000e+00 - val_output_3_accuracy: 0.0000e+00 - val_output_4_precision: 0.0000e+00 - val_output_4_recall: 0.0000e+00 - val_output_4_accuracy: 0.0000e+00 - val_output_5_precision: 0.0000e+00 - val_output_5_recall: 0.0000e+00 - val_output_5_accuracy: 0.0000e+00 - val_output_6_precision: 0.0000e+00 - val_output_6_recall: 0.0000e+00 - val_output_6_accuracy: 0.0000e+00 - val_output_7_precision: 0.0000e+00 - val_output_7_recall: 0.0000e+00 - val_output_7_accuracy: 0.0000e+00 - val_output_8_precision: 0.0000e+00 - val_output_8_recall: 0.0000e+00 - val_output_8_accuracy: 0.0000e+00 - val_output_9_precision: 0.0163 - val_output_9_recall: 0.3636 - val_output_9_accuracy: 0.0000e+00 - val_output_10_precision: 0.0266 - val_output_10_recall: 1.0000 - val_output_10_accuracy: 0.0000e+00\n",
      "763/763 [==============================] - 0s 234us/sample - loss: 6.7778 - output_1_loss: 0.6951 - output_2_loss: 0.6588 - output_3_loss: 0.6663 - output_4_loss: 0.6821 - output_5_loss: 0.6594 - output_6_loss: 0.6501 - output_7_loss: 0.6909 - output_8_loss: 0.6852 - output_9_loss: 0.6921 - output_10_loss: 0.6976 - output_1_precision: 0.0020 - output_1_recall: 1.0000 - output_1_accuracy: 0.0000e+00 - output_2_precision: 0.0000e+00 - output_2_recall: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - output_3_precision: 0.0000e+00 - output_3_recall: 0.0000e+00 - output_3_accuracy: 0.0000e+00 - output_4_precision: 0.0000e+00 - output_4_recall: 0.0000e+00 - output_4_accuracy: 0.0000e+00 - output_5_precision: 0.0000e+00 - output_5_recall: 0.0000e+00 - output_5_accuracy: 0.0000e+00 - output_6_precision: 0.0000e+00 - output_6_recall: 0.0000e+00 - output_6_accuracy: 0.0000e+00 - output_7_precision: 0.0000e+00 - output_7_recall: 0.0000e+00 - output_7_accuracy: 0.0000e+00 - output_8_precision: 0.0000e+00 - output_8_recall: 0.0000e+00 - output_8_accuracy: 0.0000e+00 - output_9_precision: 0.0163 - output_9_recall: 0.3636 - output_9_accuracy: 0.0000e+00 - output_10_precision: 0.0266 - output_10_recall: 1.0000 - output_10_accuracy: 0.0000e+00              \n",
      "hola\n",
      "Train on 3054 samples, validate on 763 samples\n",
      "3054/3054 - 10s - loss: 6.8783 - output_1_loss: 0.7019 - output_2_loss: 0.6772 - output_3_loss: 0.6803 - output_4_loss: 0.6858 - output_5_loss: 0.6779 - output_6_loss: 0.6759 - output_7_loss: 0.6994 - output_8_loss: 0.6898 - output_9_loss: 0.6921 - output_10_loss: 0.6975 - output_1_precision: 0.0057 - output_1_recall: 0.8667 - output_1_accuracy: 0.0000e+00 - output_2_precision: 0.0000e+00 - output_2_recall: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - output_3_precision: 0.0000e+00 - output_3_recall: 0.0000e+00 - output_3_accuracy: 0.0000e+00 - output_4_precision: 0.0817 - output_4_recall: 0.1186 - output_4_accuracy: 0.0000e+00 - output_5_precision: 0.0481 - output_5_recall: 0.0850 - output_5_accuracy: 0.0000e+00 - output_6_precision: 0.0644 - output_6_recall: 0.2454 - output_6_accuracy: 0.0000e+00 - output_7_precision: 0.0048 - output_7_recall: 0.7273 - output_7_accuracy: 0.0000e+00 - output_8_precision: 0.0347 - output_8_recall: 0.0302 - output_8_accuracy: 0.0000e+00 - output_9_precision: 0.0151 - output_9_recall: 0.3673 - output_9_accuracy: 0.0000e+00 - output_10_precision: 0.0200 - output_10_recall: 0.8070 - output_10_accuracy: 0.0000e+00 - val_loss: 6.7786 - val_output_1_loss: 0.6955 - val_output_2_loss: 0.6595 - val_output_3_loss: 0.6662 - val_output_4_loss: 0.6813 - val_output_5_loss: 0.6597 - val_output_6_loss: 0.6489 - val_output_7_loss: 0.6912 - val_output_8_loss: 0.6858 - val_output_9_loss: 0.6919 - val_output_10_loss: 0.6983 - val_output_1_precision: 0.0000e+00 - val_output_1_recall: 0.0000e+00 - val_output_1_accuracy: 0.0000e+00 - val_output_2_precision: 0.0000e+00 - val_output_2_recall: 0.0000e+00 - val_output_2_accuracy: 0.0000e+00 - val_output_3_precision: 0.0000e+00 - val_output_3_recall: 0.0000e+00 - val_output_3_accuracy: 0.0000e+00 - val_output_4_precision: 0.0000e+00 - val_output_4_recall: 0.0000e+00 - val_output_4_accuracy: 0.0000e+00 - val_output_5_precision: 0.0000e+00 - val_output_5_recall: 0.0000e+00 - val_output_5_accuracy: 0.0000e+00 - val_output_6_precision: 0.0000e+00 - val_output_6_recall: 0.0000e+00 - val_output_6_accuracy: 0.0000e+00 - val_output_7_precision: 0.0000e+00 - val_output_7_recall: 0.0000e+00 - val_output_7_accuracy: 0.0000e+00 - val_output_8_precision: 0.0000e+00 - val_output_8_recall: 0.0000e+00 - val_output_8_accuracy: 0.0000e+00 - val_output_9_precision: 0.0230 - val_output_9_recall: 0.3125 - val_output_9_accuracy: 0.0000e+00 - val_output_10_precision: 0.0129 - val_output_10_recall: 0.9000 - val_output_10_accuracy: 0.0000e+00\n",
      "763/763 [==============================] - 0s 237us/sample - loss: 6.7786 - output_1_loss: 0.6955 - output_2_loss: 0.6595 - output_3_loss: 0.6662 - output_4_loss: 0.6813 - output_5_loss: 0.6597 - output_6_loss: 0.6489 - output_7_loss: 0.6912 - output_8_loss: 0.6858 - output_9_loss: 0.6919 - output_10_loss: 0.6983 - output_1_precision: 0.0000e+00 - output_1_recall: 0.0000e+00 - output_1_accuracy: 0.0000e+00 - output_2_precision: 0.0000e+00 - output_2_recall: 0.0000e+00 - output_2_accuracy: 0.0000e+00 - output_3_precision: 0.0000e+00 - output_3_recall: 0.0000e+00 - output_3_accuracy: 0.0000e+00 - output_4_precision: 0.0000e+00 - output_4_recall: 0.0000e+00 - output_4_accuracy: 0.0000e+00 - output_5_precision: 0.0000e+00 - output_5_recall: 0.0000e+00 - output_5_accuracy: 0.0000e+00 - output_6_precision: 0.0000e+00 - output_6_recall: 0.0000e+00 - output_6_accuracy: 0.0000e+00 - output_7_precision: 0.0000e+00 - output_7_recall: 0.0000e+00 - output_7_accuracy: 0.0000e+00 - output_8_precision: 0.0000e+00 - output_8_recall: 0.0000e+00 - output_8_accuracy: 0.0000e+00 - output_9_precision: 0.0230 - output_9_recall: 0.3125 - output_9_accuracy: 0.0000e+00 - output_10_precision: 0.0129 - output_10_recall: 0.9000 - output_10_accuracy: 0.0000e+00      \n"
     ]
    }
   ],
   "source": [
    "for seed in seed_lst:\n",
    "    for idx,split in enumerate(train_folds):\n",
    "        # aca recuperar los folds y hacer lo de cada fold\n",
    "        print('hola')\n",
    "        sys.stdout.flush()\n",
    "        train,test = split\n",
    "        x_train, y_train = train\n",
    "        x_test, y_test = test\n",
    "        \n",
    "        # obtener params\n",
    "        for key in params_dict.keys():\n",
    "            params = params_dict.get(key)\n",
    "            lb = params.get('lb')\n",
    "            arq = params.get('arq')\n",
    "            w = params.get('w')\n",
    "            set_seed(seed)\n",
    "        \n",
    "            # entrenar modelo\n",
    "            trained_model = run_MTL(x_train, x_test, y_train, y_test, lb, arq, w)\n",
    "            \n",
    "            # computar metricas\n",
    "            pred = trained_model.evaluate(x_test, y_test)\n",
    "            # print(pred.shape)\n",
    "            sys.stdout.flush()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from MTLDNN import *\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import copy\n",
    "from timeit import default_timer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../library\")\n",
    "\n",
    "import utils\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aca la carga de los datos tokenizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    data = pd.read_csv(data_path)#.iloc[:20,:]\n",
    "    \n",
    "    # remove stopwords - tfidf whole df\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(\n",
    "        analyzer = \"word\",\n",
    "        lowercase = True,\n",
    "        tokenizer = word_tokenize,\n",
    "        stop_words = stops,\n",
    "        min_df = 5\n",
    "    )\n",
    "\n",
    "    X = vectorizer.fit_transform(data.sentence.to_numpy())\n",
    "    y = data.iloc[:,1:].astype(int).to_numpy()\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aca las fns que irian a utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_MTL(\n",
    "    X, \n",
    "    Y, \n",
    "    classifier, \n",
    "    n_splits = 5, \n",
    "    shuffle = True, \n",
    "    random_state = 1234, \n",
    "    full = False,\n",
    "    verbose = True,\n",
    "    params = None\n",
    "):\n",
    "    \n",
    "    print(\"Cross-validation process started...\")\n",
    "    start = default_timer()\n",
    "    results = []\n",
    "    data = get_folds_MTL(X, Y, n_splits, shuffle, random_state)\n",
    "    for i, ((a_train, b_train), (a_test, b_test)) in enumerate(data, 1):\n",
    "        del classifier\n",
    "        classifier = MTLDNN(1281, params)\n",
    "        classifier.compilar(params)\n",
    "        if verbose:\n",
    "            print(f\"*** fold {i} / {len(data)}\")\n",
    "            print(\"    training model...\")\n",
    "        classifier.entrenar(((a_train, b_train), (a_test, b_test)), params)\n",
    "        if verbose: \n",
    "            print(\"    generating predictions on the train set...\")\n",
    "        train_predictions = classifier.predecir(a_train)  \n",
    "        if verbose: \n",
    "            print(\"    generating predictions on the test set...\")\n",
    "        test_predictions = classifier.predecir(a_test)  \n",
    "        \n",
    "        for target in range(len(b_train)):\n",
    "            results.append(\n",
    "                dict(\n",
    "                    fold = i,\n",
    "                    column = columns.get(target),                    \n",
    "                    train_accuracy = accuracy_score(b_train[target], train_predictions[target]),\n",
    "                    train_precision = precision_score(b_train[target], train_predictions[target]),\n",
    "                    train_recall = recall_score(b_train[target], train_predictions[target]),\n",
    "                    train_f1 = f1_score(b_train[target], train_predictions[target]),\n",
    "                    test_accuracy = accuracy_score(b_test[target], test_predictions[target]),\n",
    "                    test_precision = precision_score(b_test[target], test_predictions[target]),\n",
    "                    test_recall = recall_score(b_test[target], test_predictions[target]),\n",
    "                    test_f1 = f1_score(b_test[target], test_predictions[target]),\n",
    "                )\n",
    "            )\n",
    "        # adding task_1 predictions\n",
    "        t1_train_predictions = np.logical_or.reduce(train_predictions)\n",
    "        t1_test_predictions = np.logical_or.reduce(test_predictions)\n",
    "        t1_b_train = np.logical_or.reduce(b_train)\n",
    "        t1_b_test = np.logical_or.reduce(b_test)\n",
    "        print(t1_test_predictions.shape)\n",
    "        print(t1_b_test.shape)\n",
    "        results.append(\n",
    "                dict(\n",
    "                    fold = i,\n",
    "                    column = 'task_1',                    \n",
    "                    train_accuracy = accuracy_score(t1_b_train, t1_train_predictions),\n",
    "                    train_precision = precision_score(t1_b_train, t1_train_predictions),\n",
    "                    train_recall = recall_score(t1_b_train, t1_train_predictions),\n",
    "                    train_f1 = f1_score(t1_b_train, t1_train_predictions),\n",
    "                    test_accuracy = accuracy_score(t1_b_test, t1_test_predictions),\n",
    "                    test_precision = precision_score(t1_b_test, t1_test_predictions),\n",
    "                    test_recall = recall_score(t1_b_test, t1_test_predictions),\n",
    "                    test_f1 = f1_score(t1_b_test, t1_test_predictions),\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        \n",
    "        time = default_timer() - start\n",
    "        print(f\"    Total runtime: {time/60:.2f} minutes\")\n",
    "    results = pd.DataFrame(results)\n",
    "    if full:\n",
    "        return results\n",
    "    else:\n",
    "        return results.pivot_table(\n",
    "            index = \"column\", \n",
    "            values = [\n",
    "                \"train_accuracy\", \"train_precision\", \"train_recall\", \"train_f1\", \n",
    "                \"test_accuracy\", \"test_precision\", \"test_recall\", \"test_f1\"\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same but for MTL models, which take all labels at once\n",
    "def get_folds_MTL(X, y, n_splits = 5, shuffle = True, random_state = 1234):\n",
    "    \"\"\"\n",
    "    y should be a multidimensional array of labels (for multiclass classification)\n",
    "    \"\"\"\n",
    "    kf = StratifiedKFold(\n",
    "        n_splits = n_splits, \n",
    "        shuffle = shuffle, \n",
    "        random_state = random_state\n",
    "    )\n",
    "    \n",
    "    # y_strat is the set of labels used for stratification in stratified sampling\n",
    "    # computed as the logical or among all labels in task 2 (same as task 1)\n",
    "    y_strat = np.logical_or.reduce(y.T)\n",
    "    output = []\n",
    "    for train_idx, val_idx in kf.split(X, y_strat):\n",
    "        t = (X[train_idx].toarray(), list(y[train_idx].T))\n",
    "        v = (X[val_idx].toarray(), list(y[val_idx].T))\n",
    "        output.append((t,v))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aca la instanciacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsabando/virtualenvs/tf_gpu/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "datapath =\"../../data/task_2.csv\"\n",
    "X,y = load_data(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = {\n",
    "    0:'xenophobia',\n",
    "    1:'suffering',\n",
    "    2:'economic',\n",
    "    3:'migration',\n",
    "    4:'culture',\n",
    "    5:'benefits',\n",
    "    6:'health',\n",
    "    7:'security',\n",
    "    8:'dehumanisation',\n",
    "    9:'others'}\n",
    "\n",
    "\n",
    "\n",
    "# weighed cost fn\n",
    "cw = []\n",
    "for c in list(y.T):\n",
    "    class_weights = class_weight.compute_class_weight('balanced',np.unique(y),c)\n",
    "    cw.append(class_weights)\n",
    "\n",
    "weights = {\n",
    "    'xenophobia':     {0:cw[0][0], 1:cw[0][1]},\n",
    "    'suffering':      {0:cw[1][0], 1:cw[1][1]},\n",
    "    'economic':       {0:cw[2][0], 1:cw[2][1]},\n",
    "    'migration':      {0:cw[3][0], 1:cw[3][1]},\n",
    "    'culture':        {0:cw[4][0], 1:cw[4][1]},\n",
    "    'benefits':       {0:cw[5][0], 1:cw[5][1]},\n",
    "    'health':         {0:cw[6][0], 1:cw[6][1]},\n",
    "    'security':       {0:cw[7][0], 1:cw[7][1]},\n",
    "    'dehumanisation': {0:cw[8][0], 1:cw[8][1]},\n",
    "    'others':         {0:cw[9][0], 1:cw[9][1]}\n",
    "}\n",
    "\n",
    "weights_10_outputs = {'output_1':weights.get(columns.get(0)), \n",
    "                      'output_2':weights.get(columns.get(1)), \n",
    "                      'output_3':weights.get(columns.get(2)), \n",
    "                      'output_4':weights.get(columns.get(3)), \n",
    "                      'output_5':weights.get(columns.get(4)),\n",
    "                      'output_6':weights.get(columns.get(5)), \n",
    "                      'output_7':weights.get(columns.get(6)), \n",
    "                      'output_8':weights.get(columns.get(7)), \n",
    "                      'output_9':weights.get(columns.get(8)), \n",
    "                      'output_10':weights.get(columns.get(9)),\n",
    "                     \n",
    "                     }\n",
    "\n",
    "params = {'dropout':[0.25,0.15,0.1],\n",
    "          'act':'relu',\n",
    "          'lb':0.0001,\n",
    "          'arq':(1000,500,100,20),\n",
    "          'w': weights_10_outputs,\n",
    "          'loss':BinaryCrossentropy(), \n",
    "          'l_rate':0.0001, \n",
    "          'metrics':[tf.keras.metrics.Precision(),tf.keras.metrics.Recall(),tf.keras.metrics.Accuracy()], \n",
    "          'min_delta':0.0001, \n",
    "          'patience':500,\n",
    "          'n_epochs':1000,\n",
    "          'columns': columns,\n",
    "          'momentum_batch_norm':0.9,\n",
    "          'ni':1281}\n",
    "\n",
    "mi_modelo = MTLDNN(1281, params)\n",
    "mi_modelo.compilar(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation process started...\n",
      "*** fold 1 / 5\n",
      "    training model...\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00508: early stopping\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "(764, 1)\n",
      "(764,)\n",
      "    Total runtime: 3.00 minutes\n",
      "*** fold 2 / 5\n",
      "    training model...\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00506: early stopping\n",
      "    generating predictions on the train set...\n",
      "    generating predictions on the test set...\n",
      "(764, 1)\n",
      "(764,)\n",
      "    Total runtime: 6.00 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsabando/virtualenvs/tf_gpu/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** fold 3 / 5\n",
      "    training model...\n"
     ]
    }
   ],
   "source": [
    "salida = validate_MTL(X, y, mi_modelo, n_splits = 5, shuffle = True, random_state = 1234, full = False, verbose = True, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>train_recall</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>benefits</th>\n",
       "      <td>0.834681</td>\n",
       "      <td>0.246877</td>\n",
       "      <td>0.164316</td>\n",
       "      <td>0.501390</td>\n",
       "      <td>0.843267</td>\n",
       "      <td>0.393196</td>\n",
       "      <td>0.248531</td>\n",
       "      <td>0.942301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>culture</th>\n",
       "      <td>0.800117</td>\n",
       "      <td>0.127383</td>\n",
       "      <td>0.084356</td>\n",
       "      <td>0.327593</td>\n",
       "      <td>0.820013</td>\n",
       "      <td>0.296854</td>\n",
       "      <td>0.189028</td>\n",
       "      <td>0.776141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dehumanisation</th>\n",
       "      <td>0.895211</td>\n",
       "      <td>0.043548</td>\n",
       "      <td>0.027294</td>\n",
       "      <td>0.147656</td>\n",
       "      <td>0.881319</td>\n",
       "      <td>0.219797</td>\n",
       "      <td>0.125995</td>\n",
       "      <td>0.929330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economic</th>\n",
       "      <td>0.846185</td>\n",
       "      <td>0.100372</td>\n",
       "      <td>0.057338</td>\n",
       "      <td>0.547564</td>\n",
       "      <td>0.831681</td>\n",
       "      <td>0.148452</td>\n",
       "      <td>0.080719</td>\n",
       "      <td>0.958092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>0.948392</td>\n",
       "      <td>0.035146</td>\n",
       "      <td>0.019027</td>\n",
       "      <td>0.273333</td>\n",
       "      <td>0.925529</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.064513</td>\n",
       "      <td>0.984615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>migration</th>\n",
       "      <td>0.810323</td>\n",
       "      <td>0.333048</td>\n",
       "      <td>0.237798</td>\n",
       "      <td>0.561214</td>\n",
       "      <td>0.834818</td>\n",
       "      <td>0.474518</td>\n",
       "      <td>0.323770</td>\n",
       "      <td>0.888822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>0.884203</td>\n",
       "      <td>0.076746</td>\n",
       "      <td>0.044976</td>\n",
       "      <td>0.282045</td>\n",
       "      <td>0.859838</td>\n",
       "      <td>0.196772</td>\n",
       "      <td>0.109439</td>\n",
       "      <td>0.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.770764</td>\n",
       "      <td>0.181423</td>\n",
       "      <td>0.121426</td>\n",
       "      <td>0.382598</td>\n",
       "      <td>0.797092</td>\n",
       "      <td>0.330577</td>\n",
       "      <td>0.212020</td>\n",
       "      <td>0.756886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffering</th>\n",
       "      <td>0.883152</td>\n",
       "      <td>0.074264</td>\n",
       "      <td>0.043506</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.870055</td>\n",
       "      <td>0.202092</td>\n",
       "      <td>0.113377</td>\n",
       "      <td>0.958864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task_1</th>\n",
       "      <td>0.711783</td>\n",
       "      <td>0.486785</td>\n",
       "      <td>0.441242</td>\n",
       "      <td>0.578680</td>\n",
       "      <td>0.849301</td>\n",
       "      <td>0.761675</td>\n",
       "      <td>0.681045</td>\n",
       "      <td>0.900692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xenophobia</th>\n",
       "      <td>0.833601</td>\n",
       "      <td>0.022168</td>\n",
       "      <td>0.011572</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.814392</td>\n",
       "      <td>0.053434</td>\n",
       "      <td>0.027529</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                test_accuracy   test_f1  test_precision  test_recall  \\\n",
       "column                                                                 \n",
       "benefits             0.834681  0.246877        0.164316     0.501390   \n",
       "culture              0.800117  0.127383        0.084356     0.327593   \n",
       "dehumanisation       0.895211  0.043548        0.027294     0.147656   \n",
       "economic             0.846185  0.100372        0.057338     0.547564   \n",
       "health               0.948392  0.035146        0.019027     0.273333   \n",
       "migration            0.810323  0.333048        0.237798     0.561214   \n",
       "others               0.884203  0.076746        0.044976     0.282045   \n",
       "security             0.770764  0.181423        0.121426     0.382598   \n",
       "suffering            0.883152  0.074264        0.043506     0.325000   \n",
       "task_1               0.711783  0.486785        0.441242     0.578680   \n",
       "xenophobia           0.833601  0.022168        0.011572     0.480000   \n",
       "\n",
       "                train_accuracy  train_f1  train_precision  train_recall  \n",
       "column                                                                   \n",
       "benefits              0.843267  0.393196         0.248531      0.942301  \n",
       "culture               0.820013  0.296854         0.189028      0.776141  \n",
       "dehumanisation        0.881319  0.219797         0.125995      0.929330  \n",
       "economic              0.831681  0.148452         0.080719      0.958092  \n",
       "health                0.925529  0.119400         0.064513      0.984615  \n",
       "migration             0.834818  0.474518         0.323770      0.888822  \n",
       "others                0.859838  0.196772         0.109439      0.977600  \n",
       "security              0.797092  0.330577         0.212020      0.756886  \n",
       "suffering             0.870055  0.202092         0.113377      0.958864  \n",
       "task_1                0.849301  0.761675         0.681045      0.900692  \n",
       "xenophobia            0.814392  0.053434         0.027529      1.000000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_lst = []\n",
    "params_lst.append(params)\n",
    "# salidas_lst = []\n",
    "salidas_lst.append(salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dropout': [0.25, 0.15, 0.1],\n",
       "  'act': 'relu',\n",
       "  'lb': 0.001,\n",
       "  'arq': (100, 50, 10, 5),\n",
       "  'w': {'output_1': {0: 0.5021047092870298, 1: 119.28125},\n",
       "   'output_2': {0: 0.5083910495471498, 1: 30.293650793650794},\n",
       "   'output_3': {0: 0.5073099415204678, 1: 34.7},\n",
       "   'output_4': {0: 0.5459096109839817, 1: 5.945482866043614},\n",
       "   'output_5': {0: 0.5260474090407938, 1: 10.097883597883598},\n",
       "   'output_6': {0: 0.528523954583218, 1: 9.264563106796116},\n",
       "   'output_7': {0: 0.5022368421052632, 1: 112.26470588235294},\n",
       "   'output_8': {0: 0.5357944974733296, 1: 7.484313725490196},\n",
       "   'output_9': {0: 0.5086620469083155, 1: 29.361538461538462},\n",
       "   'output_10': {0: 0.5089333333333333, 1: 28.48507462686567}},\n",
       "  'loss': <tensorflow.python.keras.losses.BinaryCrossentropy at 0x147f25f998d0>,\n",
       "  'l_rate': 0.0001,\n",
       "  'metrics': [<tensorflow.python.keras.metrics.Precision at 0x147e6b130b10>,\n",
       "   <tensorflow.python.keras.metrics.Recall at 0x147e6b096c50>,\n",
       "   <tensorflow.python.keras.metrics.Accuracy at 0x147e6b0964d0>],\n",
       "  'min_delta': 0.0001,\n",
       "  'patience': 500,\n",
       "  'n_epochs': 1000,\n",
       "  'columns': {0: 'xenophobia',\n",
       "   1: 'suffering',\n",
       "   2: 'economic',\n",
       "   3: 'migration',\n",
       "   4: 'culture',\n",
       "   5: 'benefits',\n",
       "   6: 'health',\n",
       "   7: 'security',\n",
       "   8: 'dehumanisation',\n",
       "   9: 'others'},\n",
       "  'momentum_batch_norm': 0.9,\n",
       "  'ni': 1281},\n",
       " {'dropout': [0.25, 0.15, 0.1],\n",
       "  'act': 'relu',\n",
       "  'lb': 0.001,\n",
       "  'arq': (200, 100, 20, 10),\n",
       "  'w': {'output_1': {0: 0.5021047092870298, 1: 119.28125},\n",
       "   'output_2': {0: 0.5083910495471498, 1: 30.293650793650794},\n",
       "   'output_3': {0: 0.5073099415204678, 1: 34.7},\n",
       "   'output_4': {0: 0.5459096109839817, 1: 5.945482866043614},\n",
       "   'output_5': {0: 0.5260474090407938, 1: 10.097883597883598},\n",
       "   'output_6': {0: 0.528523954583218, 1: 9.264563106796116},\n",
       "   'output_7': {0: 0.5022368421052632, 1: 112.26470588235294},\n",
       "   'output_8': {0: 0.5357944974733296, 1: 7.484313725490196},\n",
       "   'output_9': {0: 0.5086620469083155, 1: 29.361538461538462},\n",
       "   'output_10': {0: 0.5089333333333333, 1: 28.48507462686567}},\n",
       "  'loss': <tensorflow.python.keras.losses.BinaryCrossentropy at 0x1473ce452b10>,\n",
       "  'l_rate': 1e-05,\n",
       "  'metrics': [<tensorflow.python.keras.metrics.Precision at 0x1473ce3e2650>,\n",
       "   <tensorflow.python.keras.metrics.Recall at 0x1473ce3f7dd0>,\n",
       "   <tensorflow.python.keras.metrics.Accuracy at 0x1473ce3fc250>],\n",
       "  'min_delta': 0.0001,\n",
       "  'patience': 100,\n",
       "  'n_epochs': 1000,\n",
       "  'columns': {0: 'xenophobia',\n",
       "   1: 'suffering',\n",
       "   2: 'economic',\n",
       "   3: 'migration',\n",
       "   4: 'culture',\n",
       "   5: 'benefits',\n",
       "   6: 'health',\n",
       "   7: 'security',\n",
       "   8: 'dehumanisation',\n",
       "   9: 'others'},\n",
       "  'momentum_batch_norm': 0.9,\n",
       "  'ni': 1281},\n",
       " {'dropout': [0.25, 0.15, 0.1],\n",
       "  'act': 'relu',\n",
       "  'lb': 0.005,\n",
       "  'arq': (500, 200, 100, 50),\n",
       "  'w': {'output_1': {0: 0.5021047092870298, 1: 119.28125},\n",
       "   'output_2': {0: 0.5083910495471498, 1: 30.293650793650794},\n",
       "   'output_3': {0: 0.5073099415204678, 1: 34.7},\n",
       "   'output_4': {0: 0.5459096109839817, 1: 5.945482866043614},\n",
       "   'output_5': {0: 0.5260474090407938, 1: 10.097883597883598},\n",
       "   'output_6': {0: 0.528523954583218, 1: 9.264563106796116},\n",
       "   'output_7': {0: 0.5022368421052632, 1: 112.26470588235294},\n",
       "   'output_8': {0: 0.5357944974733296, 1: 7.484313725490196},\n",
       "   'output_9': {0: 0.5086620469083155, 1: 29.361538461538462},\n",
       "   'output_10': {0: 0.5089333333333333, 1: 28.48507462686567}},\n",
       "  'loss': <tensorflow.python.keras.losses.BinaryCrossentropy at 0x1473ce43d890>,\n",
       "  'l_rate': 1e-05,\n",
       "  'metrics': [<tensorflow.python.keras.metrics.Precision at 0x147e5143db90>,\n",
       "   <tensorflow.python.keras.metrics.Recall at 0x147e6b0e4650>,\n",
       "   <tensorflow.python.keras.metrics.Accuracy at 0x147e6b0e49d0>],\n",
       "  'min_delta': 0.0001,\n",
       "  'patience': 100,\n",
       "  'n_epochs': 1000,\n",
       "  'columns': {0: 'xenophobia',\n",
       "   1: 'suffering',\n",
       "   2: 'economic',\n",
       "   3: 'migration',\n",
       "   4: 'culture',\n",
       "   5: 'benefits',\n",
       "   6: 'health',\n",
       "   7: 'security',\n",
       "   8: 'dehumanisation',\n",
       "   9: 'others'},\n",
       "  'momentum_batch_norm': 0.9,\n",
       "  'ni': 1281},\n",
       " {'dropout': [0.25, 0.15, 0.1],\n",
       "  'act': 'relu',\n",
       "  'lb': 0.005,\n",
       "  'arq': (500, 200, 100, 50),\n",
       "  'w': {'output_1': {0: 0.5021047092870298, 1: 119.28125},\n",
       "   'output_2': {0: 0.5083910495471498, 1: 30.293650793650794},\n",
       "   'output_3': {0: 0.5073099415204678, 1: 34.7},\n",
       "   'output_4': {0: 0.5459096109839817, 1: 5.945482866043614},\n",
       "   'output_5': {0: 0.5260474090407938, 1: 10.097883597883598},\n",
       "   'output_6': {0: 0.528523954583218, 1: 9.264563106796116},\n",
       "   'output_7': {0: 0.5022368421052632, 1: 112.26470588235294},\n",
       "   'output_8': {0: 0.5357944974733296, 1: 7.484313725490196},\n",
       "   'output_9': {0: 0.5086620469083155, 1: 29.361538461538462},\n",
       "   'output_10': {0: 0.5089333333333333, 1: 28.48507462686567}},\n",
       "  'loss': <tensorflow.python.keras.losses.BinaryCrossentropy at 0x147e6bc37110>,\n",
       "  'l_rate': 1e-05,\n",
       "  'metrics': [<tensorflow.python.keras.metrics.Precision at 0x1473d7a8d6d0>,\n",
       "   <tensorflow.python.keras.metrics.Recall at 0x1473d7a9aad0>,\n",
       "   <tensorflow.python.keras.metrics.Accuracy at 0x1473d7ab0410>],\n",
       "  'min_delta': 0.0001,\n",
       "  'patience': 500,\n",
       "  'n_epochs': 1000,\n",
       "  'columns': {0: 'xenophobia',\n",
       "   1: 'suffering',\n",
       "   2: 'economic',\n",
       "   3: 'migration',\n",
       "   4: 'culture',\n",
       "   5: 'benefits',\n",
       "   6: 'health',\n",
       "   7: 'security',\n",
       "   8: 'dehumanisation',\n",
       "   9: 'others'},\n",
       "  'momentum_batch_norm': 0.9,\n",
       "  'ni': 1281},\n",
       " {'dropout': [0.25, 0.15, 0.1],\n",
       "  'act': 'relu',\n",
       "  'lb': 0.005,\n",
       "  'arq': (50, 20, 10, 5),\n",
       "  'w': {'output_1': {0: 0.5021047092870298, 1: 119.28125},\n",
       "   'output_2': {0: 0.5083910495471498, 1: 30.293650793650794},\n",
       "   'output_3': {0: 0.5073099415204678, 1: 34.7},\n",
       "   'output_4': {0: 0.5459096109839817, 1: 5.945482866043614},\n",
       "   'output_5': {0: 0.5260474090407938, 1: 10.097883597883598},\n",
       "   'output_6': {0: 0.528523954583218, 1: 9.264563106796116},\n",
       "   'output_7': {0: 0.5022368421052632, 1: 112.26470588235294},\n",
       "   'output_8': {0: 0.5357944974733296, 1: 7.484313725490196},\n",
       "   'output_9': {0: 0.5086620469083155, 1: 29.361538461538462},\n",
       "   'output_10': {0: 0.5089333333333333, 1: 28.48507462686567}},\n",
       "  'loss': <tensorflow.python.keras.losses.BinaryCrossentropy at 0x1473d7b62fd0>,\n",
       "  'l_rate': 1e-05,\n",
       "  'metrics': [<tensorflow.python.keras.metrics.Precision at 0x147d6c4608d0>,\n",
       "   <tensorflow.python.keras.metrics.Recall at 0x147d6c477110>,\n",
       "   <tensorflow.python.keras.metrics.Accuracy at 0x147d6c4775d0>],\n",
       "  'min_delta': 0.0001,\n",
       "  'patience': 500,\n",
       "  'n_epochs': 1000,\n",
       "  'columns': {0: 'xenophobia',\n",
       "   1: 'suffering',\n",
       "   2: 'economic',\n",
       "   3: 'migration',\n",
       "   4: 'culture',\n",
       "   5: 'benefits',\n",
       "   6: 'health',\n",
       "   7: 'security',\n",
       "   8: 'dehumanisation',\n",
       "   9: 'others'},\n",
       "  'momentum_batch_norm': 0.9,\n",
       "  'ni': 1281}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_lst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
